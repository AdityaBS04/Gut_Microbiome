{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gut Microbiome Disease Risk Prediction Model\n",
    "Implementation of encoder-decoder VAE for predicting disease risk from microbiome data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install pandas numpy scikit-learn torch torchvision tqdm\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "np.random.seed(42)\n",
    "import torch\n",
    "torch.manual_seed(42)\n",
    "\n",
    "print(\"✓ Dependencies loaded successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Loading Class\n",
    "class DataLoader:\n",
    "    def __init__(self, data_dir=\"./\"):\n",
    "        self.data_dir = Path(data_dir)\n",
    "        self.disease_files = {\n",
    "            'adenoma': 'Adenoma.csv',\n",
    "            'alzheimer': 'Alzheimer_Disease.csv',\n",
    "            'anemia_sickle': 'Anemia_Sickle_Cell.csv',\n",
    "            'anorexia': 'Anorexia.csv',\n",
    "            'arthritis_juvenile': 'Arthritis_Juvenile.csv',\n",
    "            'arthritis_reactive': 'Arthritis_Reactive.csv',\n",
    "            'arthritis_rheumatoid': 'Arthritis_Rheumatoid.csv',\n",
    "            'asthma': 'Asthma.csv',\n",
    "            'atherosclerosis': 'Atherosclerosis.csv',\n",
    "            'adhd': 'Attention_Deficit_Disorder_with_Hyperactivity.csv'\n",
    "        }\n",
    "        self.data = {}\n",
    "        self.combined_data = None\n",
    "        \n",
    "    def load_single_file(self, disease_name):\n",
    "        file_path = self.data_dir / self.disease_files[disease_name]\n",
    "        if file_path.exists():\n",
    "            df = pd.read_csv(file_path)\n",
    "            df['disease_label'] = disease_name\n",
    "            df['is_healthy'] = 0\n",
    "            print(f\"✓ Loaded {disease_name}: {len(df)} samples\")\n",
    "            return df\n",
    "        else:\n",
    "            print(f\"✗ File not found: {file_path}\")\n",
    "            return None\n",
    "    \n",
    "    def load_all_files(self):\n",
    "        for disease_name in self.disease_files.keys():\n",
    "            self.data[disease_name] = self.load_single_file(disease_name)\n",
    "        \n",
    "        valid_data = [df for df in self.data.values() if df is not None]\n",
    "        self.combined_data = pd.concat(valid_data, ignore_index=True)\n",
    "        print(f\"\\n✓ Total samples loaded: {len(self.combined_data)}\")\n",
    "        return self.combined_data\n",
    "\n",
    "# Load data\n",
    "loader = DataLoader()\n",
    "combined_data = loader.load_all_files()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create abundance matrix\n",
    "print(\"Creating abundance matrix...\")\n",
    "abundance_matrix = combined_data.pivot_table(\n",
    "    index='run_id',\n",
    "    columns='scientific_name',\n",
    "    values='relative_abundance',\n",
    "    aggfunc='mean',\n",
    "    fill_value=0\n",
    ")\n",
    "\n",
    "# Filter low-prevalence species (>5% prevalence)\n",
    "prevalence = (abundance_matrix > 0).mean()\n",
    "keep_species = prevalence[prevalence >= 0.05].index\n",
    "abundance_matrix = abundance_matrix[keep_species]\n",
    "\n",
    "print(f\"✓ Abundance matrix shape: {abundance_matrix.shape}\")\n",
    "\n",
    "# Create sample metadata\n",
    "sample_metadata = combined_data.groupby('run_id').agg({\n",
    "    'disease_label': 'first',\n",
    "    'is_healthy': 'first',\n",
    "    'host_age': 'mean',\n",
    "    'sex': 'first',\n",
    "    'country': 'first'\n",
    "}).loc[abundance_matrix.index]\n",
    "\n",
    "print(f\"✓ Sample metadata shape: {sample_metadata.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply CLR transformation\n",
    "print(\"Applying CLR transformation...\")\n",
    "pseudocount = 1e-6\n",
    "data_pseudo = abundance_matrix + pseudocount\n",
    "geometric_mean = np.exp(np.log(data_pseudo).mean(axis=1))\n",
    "abundance_matrix_clr = np.log(data_pseudo.div(geometric_mean, axis=0))\n",
    "\n",
    "# Handle missing values\n",
    "from sklearn.experimental import enable_iterative_imputer\n",
    "from sklearn.impute import IterativeImputer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "imputer = IterativeImputer(random_state=42, max_iter=5)\n",
    "abundance_imputed = pd.DataFrame(\n",
    "    imputer.fit_transform(abundance_matrix_clr),\n",
    "    index=abundance_matrix_clr.index,\n",
    "    columns=abundance_matrix_clr.columns\n",
    ")\n",
    "\n",
    "# Scale features\n",
    "scaler = StandardScaler()\n",
    "abundance_scaled = pd.DataFrame(\n",
    "    scaler.fit_transform(abundance_imputed),\n",
    "    index=abundance_imputed.index,\n",
    "    columns=abundance_imputed.columns\n",
    ")\n",
    "\n",
    "print(f\"✓ Processed data shape: {abundance_scaled.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature selection\n",
    "from sklearn.feature_selection import SelectKBest, f_classif\n",
    "\n",
    "# Create binary disease labels\n",
    "disease_labels = pd.get_dummies(sample_metadata['disease_label'])\n",
    "\n",
    "# Select top features based on average scores across diseases\n",
    "scores_per_label = []\n",
    "for disease_col in disease_labels.columns:\n",
    "    selector = SelectKBest(f_classif, k=min(100, abundance_scaled.shape[1]))\n",
    "    selector.fit(abundance_scaled, disease_labels[disease_col])\n",
    "    scores_per_label.append(selector.scores_)\n",
    "\n",
    "avg_scores = np.mean(scores_per_label, axis=0)\n",
    "top_k_indices = np.argsort(avg_scores)[-100:]\n",
    "selected_features = abundance_scaled.columns[top_k_indices]\n",
    "X_selected = abundance_scaled[selected_features]\n",
    "\n",
    "print(f\"✓ Selected {len(selected_features)} features\")\n",
    "print(f\"✓ Final feature matrix shape: {X_selected.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create risk scores (continuous targets)\n",
    "risk_scores = disease_labels.copy().astype(float)\n",
    "\n",
    "# For disease samples: high risk (0.8-1.0) for their disease, low for others\n",
    "# For simplicity, we'll use binary labels with some noise\n",
    "for idx in risk_scores.index:\n",
    "    current_scores = risk_scores.loc[idx].values\n",
    "    disease_indices = np.where(current_scores == 1)[0]\n",
    "    \n",
    "    # Primary disease gets high score\n",
    "    for disease_idx in disease_indices:\n",
    "        current_scores[disease_idx] = np.random.uniform(0.8, 1.0)\n",
    "    \n",
    "    # Other diseases get low scores\n",
    "    other_indices = np.where(current_scores == 0)[0]\n",
    "    for other_idx in other_indices:\n",
    "        current_scores[other_idx] = np.random.uniform(0, 0.2)\n",
    "    \n",
    "    risk_scores.loc[idx] = current_scores\n",
    "\n",
    "print(f\"✓ Risk scores created: {risk_scores.shape}\")\n",
    "print(f\"✓ Risk score range: [{risk_scores.min().min():.3f}, {risk_scores.max().max():.3f}]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Architecture\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class MicrobiomeVAE(nn.Module):\n",
    "    def __init__(self, input_dim, latent_dim=32, n_diseases=10):\n",
    "        super(MicrobiomeVAE, self).__init__()\n",
    "        \n",
    "        # Encoder\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(input_dim, 256),\n",
    "            nn.BatchNorm1d(256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(256, 128),\n",
    "            nn.BatchNorm1d(128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2)\n",
    "        )\n",
    "        \n",
    "        self.fc_mu = nn.Linear(128, latent_dim)\n",
    "        self.fc_logvar = nn.Linear(128, latent_dim)\n",
    "        \n",
    "        # Decoder\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(latent_dim, 128),\n",
    "            nn.BatchNorm1d(128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(128, 256),\n",
    "            nn.BatchNorm1d(256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(256, n_diseases),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "        \n",
    "    def reparameterize(self, mu, logvar):\n",
    "        std = torch.exp(0.5 * logvar)\n",
    "        eps = torch.randn_like(std)\n",
    "        return mu + eps * std\n",
    "    \n",
    "    def forward(self, x):\n",
    "        h = self.encoder(x)\n",
    "        mu = self.fc_mu(h)\n",
    "        logvar = self.fc_logvar(h)\n",
    "        z = self.reparameterize(mu, logvar)\n",
    "        risk_scores = self.decoder(z)\n",
    "        return risk_scores, mu, logvar, z\n",
    "\n",
    "# Initialize model\n",
    "input_dim = X_selected.shape[1]\n",
    "n_diseases = risk_scores.shape[1]\n",
    "model = MicrobiomeVAE(input_dim=input_dim, latent_dim=32, n_diseases=n_diseases)\n",
    "\n",
    "print(f\"✓ Model initialized:\")\n",
    "print(f\"  Input dimension: {input_dim}\")\n",
    "print(f\"  Output diseases: {n_diseases}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data preparation for training\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import TensorDataset, DataLoader as TorchDataLoader\n",
    "\n",
    "# Split data\n",
    "stratify_labels = sample_metadata['disease_label'].values\n",
    "\n",
    "X_temp, X_test, y_temp, y_test, meta_temp, meta_test = train_test_split(\n",
    "    X_selected, risk_scores, sample_metadata,\n",
    "    test_size=0.2, random_state=42, stratify=stratify_labels\n",
    ")\n",
    "\n",
    "stratify_temp = meta_temp['disease_label'].values\n",
    "X_train, X_val, y_train, y_val, meta_train, meta_val = train_test_split(\n",
    "    X_temp, y_temp, meta_temp,\n",
    "    test_size=0.25, random_state=42, stratify=stratify_temp\n",
    ")\n",
    "\n",
    "print(f\"✓ Train set: {X_train.shape[0]} samples\")\n",
    "print(f\"✓ Val set: {X_val.shape[0]} samples\")\n",
    "print(f\"✓ Test set: {X_test.shape[0]} samples\")\n",
    "\n",
    "# Create data loaders\n",
    "train_dataset = TensorDataset(\n",
    "    torch.FloatTensor(X_train.values),\n",
    "    torch.FloatTensor(y_train.values)\n",
    ")\n",
    "val_dataset = TensorDataset(\n",
    "    torch.FloatTensor(X_val.values),\n",
    "    torch.FloatTensor(y_val.values)\n",
    ")\n",
    "test_dataset = TensorDataset(\n",
    "    torch.FloatTensor(X_test.values),\n",
    "    torch.FloatTensor(y_test.values)\n",
    ")\n",
    "\n",
    "train_loader = TorchDataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "val_loader = TorchDataLoader(val_dataset, batch_size=64, shuffle=False)\n",
    "test_loader = TorchDataLoader(test_dataset, batch_size=64, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss function\n",
    "class VAELoss(nn.Module):\n",
    "    def __init__(self, beta=0.5):\n",
    "        super(VAELoss, self).__init__()\n",
    "        self.beta = beta\n",
    "        \n",
    "    def forward(self, risk_pred, risk_true, mu, logvar):\n",
    "        # Reconstruction loss\n",
    "        recon_loss = F.mse_loss(risk_pred, risk_true, reduction='mean')\n",
    "        \n",
    "        # KL divergence\n",
    "        kl_loss = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
    "        kl_loss = kl_loss / risk_pred.shape[0]\n",
    "        \n",
    "        total_loss = recon_loss + self.beta * kl_loss\n",
    "        return total_loss, recon_loss, kl_loss\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "model = model.to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "loss_fn = VAELoss(beta=0.5)\n",
    "\n",
    "print(f\"✓ Training setup complete on device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training loop\n",
    "from tqdm import tqdm\n",
    "\n",
    "def train_epoch(model, train_loader, optimizer, loss_fn, device):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    total_recon = 0\n",
    "    total_kl = 0\n",
    "    \n",
    "    for batch_x, batch_y in train_loader:\n",
    "        batch_x, batch_y = batch_x.to(device), batch_y.to(device)\n",
    "        \n",
    "        risk_pred, mu, logvar, _ = model(batch_x)\n",
    "        loss, recon_loss, kl_loss = loss_fn(risk_pred, batch_y, mu, logvar)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "        total_recon += recon_loss.item()\n",
    "        total_kl += kl_loss.item()\n",
    "    \n",
    "    n_batches = len(train_loader)\n",
    "    return total_loss/n_batches, total_recon/n_batches, total_kl/n_batches\n",
    "\n",
    "def validate(model, val_loader, loss_fn, device):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    total_recon = 0\n",
    "    total_kl = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch_x, batch_y in val_loader:\n",
    "            batch_x, batch_y = batch_x.to(device), batch_y.to(device)\n",
    "            risk_pred, mu, logvar, _ = model(batch_x)\n",
    "            loss, recon_loss, kl_loss = loss_fn(risk_pred, batch_y, mu, logvar)\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "            total_recon += recon_loss.item()\n",
    "            total_kl += kl_loss.item()\n",
    "    \n",
    "    n_batches = len(val_loader)\n",
    "    return total_loss/n_batches, total_recon/n_batches, total_kl/n_batches\n",
    "\n",
    "# Training loop\n",
    "epochs = 30\n",
    "best_val_loss = float('inf')\n",
    "patience = 8\n",
    "patience_counter = 0\n",
    "\n",
    "print(\"Starting training...\")\n",
    "print(\"Epoch | Train Loss | Val Loss   | Train Recon| Val Recon  | Train KL   | Val KL\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    train_loss, train_recon, train_kl = train_epoch(model, train_loader, optimizer, loss_fn, device)\n",
    "    val_loss, val_recon, val_kl = validate(model, val_loader, loss_fn, device)\n",
    "    \n",
    "    print(f\"{epoch+1:5d} | {train_loss:10.4f} | {val_loss:10.4f} | {train_recon:10.4f} | {val_recon:10.4f} | {train_kl:10.4f} | {val_kl:10.4f}\")\n",
    "    \n",
    "    # Early stopping\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        patience_counter = 0\n",
    "        torch.save(model.state_dict(), 'best_model.pth')\n",
    "    else:\n",
    "        patience_counter += 1\n",
    "        if patience_counter >= patience:\n",
    "            print(f\"\\nEarly stopping at epoch {epoch+1}\")\n",
    "            break\n",
    "\n",
    "# Load best model\n",
    "model.load_state_dict(torch.load('best_model.pth'))\n",
    "print(\"\\n✓ Training completed and best model loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model evaluation\n",
    "from sklearn.metrics import roc_auc_score, average_precision_score, mean_squared_error, classification_report\n",
    "\n",
    "def evaluate_model(model, test_loader, device):\n",
    "    model.eval()\n",
    "    predictions = []\n",
    "    actuals = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch_x, batch_y in test_loader:\n",
    "            batch_x = batch_x.to(device)\n",
    "            risk_pred, _, _, _ = model(batch_x)\n",
    "            predictions.append(risk_pred.cpu().numpy())\n",
    "            actuals.append(batch_y.numpy())\n",
    "    \n",
    "    predictions = np.vstack(predictions)\n",
    "    actuals = np.vstack(actuals)\n",
    "    \n",
    "    return predictions, actuals\n",
    "\n",
    "# Get predictions\n",
    "test_pred, test_actual = evaluate_model(model, test_loader, device)\n",
    "\n",
    "# Calculate metrics\n",
    "print(\"MODEL EVALUATION RESULTS\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Overall MSE\n",
    "overall_mse = mean_squared_error(test_actual, test_pred)\n",
    "print(f\"\\nOverall MSE: {overall_mse:.4f}\")\n",
    "\n",
    "# Per-disease metrics\n",
    "disease_names = risk_scores.columns.tolist()\n",
    "print(\"\\nPer-Disease Performance:\")\n",
    "print(\"-\" * 60)\n",
    "print(f\"{'Disease':<25} {'AUC':>8} {'AP':>8} {'MSE':>8} {'F1':>8}\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "for i, disease in enumerate(disease_names):\n",
    "    # Convert to binary for classification metrics\n",
    "    y_true_binary = (test_actual[:, i] > 0.5).astype(int)\n",
    "    y_pred_binary = (test_pred[:, i] > 0.5).astype(int)\n",
    "    \n",
    "    try:\n",
    "        auc = roc_auc_score(y_true_binary, test_pred[:, i])\n",
    "        ap = average_precision_score(y_true_binary, test_pred[:, i])\n",
    "        # F1 score\n",
    "        from sklearn.metrics import f1_score\n",
    "        f1 = f1_score(y_true_binary, y_pred_binary)\n",
    "    except:\n",
    "        auc = np.nan\n",
    "        ap = np.nan\n",
    "        f1 = np.nan\n",
    "    \n",
    "    mse_disease = mean_squared_error(test_actual[:, i], test_pred[:, i])\n",
    "    \n",
    "    print(f\"{disease:<25} {auc:>8.3f} {ap:>8.3f} {mse_disease:>8.4f} {f1:>8.3f}\")\n",
    "\n",
    "# Summary statistics\n",
    "print(\"\\nSUMMARY STATISTICS\")\n",
    "print(\"=\" * 30)\n",
    "print(f\"Average prediction: {test_pred.mean():.3f}\")\n",
    "print(f\"Prediction std: {test_pred.std():.3f}\")\n",
    "print(f\"Prediction range: [{test_pred.min():.3f}, {test_pred.max():.3f}]\")\n",
    "\n",
    "# Calculate accuracy for binary classification (threshold = 0.5)\n",
    "binary_pred = (test_pred > 0.5).astype(int)\n",
    "binary_actual = (test_actual > 0.5).astype(int)\n",
    "accuracy = (binary_pred == binary_actual).mean()\n",
    "print(f\"Overall binary accuracy: {accuracy:.3f}\")\n",
    "\n",
    "print(\"\\n✓ Evaluation complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save model for deployment\n",
    "import pickle\n",
    "\n",
    "# Save model checkpoint\n",
    "torch.save({\n",
    "    'model_state_dict': model.state_dict(),\n",
    "    'model_config': {\n",
    "        'input_dim': input_dim,\n",
    "        'latent_dim': 32,\n",
    "        'n_diseases': n_diseases\n",
    "    }\n",
    "}, 'microbiome_model.pth')\n",
    "\n",
    "# Save preprocessing artifacts\n",
    "with open('preprocessing_artifacts.pkl', 'wb') as f:\n",
    "    pickle.dump({\n",
    "        'scaler': scaler,\n",
    "        'selected_features': selected_features.tolist(),\n",
    "        'disease_names': disease_names\n",
    "    }, f)\n",
    "\n",
    "print(\"✓ Model and preprocessing artifacts saved\")\n",
    "print(\"  - microbiome_model.pth\")\n",
    "print(\"  - preprocessing_artifacts.pkl\")\n",
    "\n",
    "# Display final model architecture summary\n",
    "print(\"\\nFINAL MODEL SUMMARY\")\n",
    "print(\"=\" * 30)\n",
    "print(f\"Input features: {input_dim}\")\n",
    "print(f\"Latent dimension: 32\")\n",
    "print(f\"Output diseases: {n_diseases}\")\n",
    "print(f\"Total parameters: {sum(p.numel() for p.numel() in model.parameters()):,}\")\n",
    "print(f\"Trainable parameters: {sum(p.numel() for p in model.parameters() if p.requires_grad):,}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}